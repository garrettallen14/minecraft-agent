{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perception_llms\n",
    "import json\n",
    "from javascript import require, On, Once, AsyncTask, once, off, globalThis\n",
    "findAndParseJsonLikeText = require('json-like-parse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = perception_llms.get_llms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task='Is there a river nearby?'\n"
     ]
    }
   ],
   "source": [
    "task = \"Is there a river nearby?\"\n",
    "perception_functions = json.loads(open('perception_functions.json').read())\n",
    "print(f'{task=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_reasoning_modules='9 How can I break down this problem into smaller, more manageable parts?\\n20 Are there any relevant data or information that can provide insights into the problem? If yes, what data sources are available, and how can they be analyzed?\\n26 Does the problem involve a physical constraint, such as limited resources, infrastructure, or space?'\n"
     ]
    }
   ],
   "source": [
    "relevant_reasoning_modules = llms['select'].invoke({\n",
    "    'task': task\n",
    "}).content\n",
    "print(f'{relevant_reasoning_modules=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapted_reasoning_modules='\\n9 How can I break down this problem into smaller, more manageable parts in Minecraft?\\n- Break down the problem by exploring the surrounding area and identifying potential signs of a river, such as flowing water or a biome that typically contains rivers.\\n\\n20 Are there any relevant data or information that can provide insights into the presence of a river in Minecraft? If yes, what data sources are available, and how can they be analyzed?\\n- Utilize in-game maps, coordinates, and biome information to analyze the terrain and determine the likelihood of a river nearby.\\n\\n26 Does the problem involve a physical constraint, such as limited resources, infrastructure, or space, that may affect the search for a river in Minecraft?\\n- Consider the limitations of your resources, such as tools for exploration or transportation methods, that may impact your ability to locate a river in the game.'\n",
      "implemented_reasoning_modules='{\\n    \"Search for flowing water or biome indicators that typically contain rivers, such as plains or forest biomes.\": \"\",\\n    \"Utilize in-game maps and coordinates to identify potential river locations based on terrain features.\": \"\",\\n    \"Consider the limitations of available resources and tools for exploration that may impact the search for a river.\": \"\"\\n}'\n"
     ]
    }
   ],
   "source": [
    "adapted_reasoning_modules = llms['adapt'].invoke({\n",
    "    'task': task,\n",
    "    'relevant_reasoning_modules': relevant_reasoning_modules\n",
    "}).content\n",
    "print(f'{adapted_reasoning_modules=}')\n",
    "implemented_reasoning_modules = llms['implement'].invoke({\n",
    "    'task': task,\n",
    "    'adapted_reasoning_modules': adapted_reasoning_modules,\n",
    "    'perception_functions': perception_functions\n",
    "}).content\n",
    "print(f'{implemented_reasoning_modules=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  'Search for flowing water or biome indicators that typically contain rivers, such as plains or forest biomes.': '',\n",
       "  'Utilize in-game maps and coordinates to identify potential river locations based on terrain features.': '',\n",
       "  'Consider the limitations of available resources and tools for exploration that may impact the search for a river.': ''\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtasks = findAndParseJsonLikeText(implemented_reasoning_modules)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['attempt', 'environment', 'perception_functions', 'subtasks', 'task'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['attempt', 'environment', 'perception_functions', 'subtasks', 'task'], template='SYSTEM: You are a Minecraft Task Completion Assistant. You are given a task to complete in Minecraft.\\nYou have been given a Task to complete.\\nYou have generated some Subtasks to solve the main Task.\\nYour current goal is to perceive the Environment and solve the Subtask.\\n\\nYou have two Options for your RESPONSE:\\nOption 1: Respond with \"SUBTASK ANSWER: ...\" (Your answer to a Subtask, using the relevant information from the Environment)\\nOption 2: Respond only with \"functionCall(bot, \\'xyz\\')\" (A single perception function call to add to your perception of the Environment and help solve the Subtask in the following iteration)\\n\\n# If you decide to use Option 1, respond with the entire JSON structure of the Subtasks with the correct answers.\\nExample:\\nBefore:\\nSubtasks = \"subtask1\": \"\", \"subtask2\": \"\", \"subtask3\": \"\"\\nAfter:\\nRESPONSE: SUBTASK ANSWER: \"subtask1\": \"answer1\", \"subtask2\": \"\", \"subtask3\": \"answer3\" (in proper JSON format)\\n# If you decide to use Option 2, respond with a single function call to add to your perception of the Environment and help solve the Subtask in the following iteration.\\nExample:\\nRESPONSE: functionCall(bot, \\'xyz\\')\\n\\n...\\n\\nTask: {task}\\nSubtasks: {subtasks}\\nEnvironment: {environment}\\n\\nPERCEPTION FUNCTIONS:\\n{perception_functions}\\n\\nImportant: You only have 5 attempts to find all SUBTASK ANSWERs. On Attempt = 5, you must fill in as many SUBTASK ANSWERs as you possibly can.\\nCurrent Attempt: {attempt}\\n# Choose the best Option for your Response and fill in the SUBTASK ANSWERs or call a perception function to execute to enhance your perception of the Environment.\\nRESPONSE: '))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000267FA974C10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000267FA9A8CD0>, model_name='gpt-3.5-turbo-0125', temperature=0.1, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llms['perceive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = str(getPromptInfo())\n",
    "attempt = 1\n",
    "while attempt <= 5:\n",
    "    response = llms['perceive'].invoke({\n",
    "        'task': task,\n",
    "        'subtasks': implemented_reasoning_modules,\n",
    "        'environment': environment,\n",
    "        'attempt': attempt,\n",
    "        'perception_functions': perception_functions\n",
    "    }).content\n",
    "    print(f'{response=}')\n",
    "\n",
    "    attempt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
